Raspberry Pi Synth Project
==========================

Underway
========
* Implement envelope rendering
* Implement envelope controls
  * Add decay, sustain & release controls.
  * Link control effects to keep envelope.
  * Make controller mechanism data-driven.

To do next
==========
* Create types for fixed values, and also some basic math funcs/macros for readability
* Investigate little audible difference in 50% cut from A -> D -> S - maybe attack not getting there fully?
* Implement envelope release (voices need to run on...) 

Blog ideas
==========
* New case
* Basic architecture and operation
* On the quality of digital audio

Ideas to try
============
Feature support:
 * Rendering:
   * Of envelope shapes
   * Of settings
 * Better naming for oscillators & generators
 * Normalise range of phase accumulator across all waveform types
 * Multi-wave wavetable support 
 * Cubic interpolation with table lookup
 * Implementation of "whole synth" (see overall structure notes below)
 * Paired oscillators: modulation, mixing
 * Modulation tools:
   * Envelope generator
   * Amplitude modulation
   * Frequency modulation
   * LFOs
 * Noise channel
 * Filtering

Performance:
 * More profiling of waveform generation loops
 * Examine asm code generated for waveform gen loops; look for calls vs inlined code
 * Float based implementation (wavetable & procedural)
   * Rationales:
     * See if it produces output closer to sine for the approximate sine procedural
     * See if it produces less aliased output for saw wave (wavetable or procedural)
 * Threading setup
   * Move "synth engine" to its own thread, allow main thread to be idle?
   * Set non-main thread priorities accordingly.

Advanced:
 * Phase Distortion control (simple linear)

Pitch Difference Issue (resolved)
----------------------
* Procedural generated waveform is pitching lower than wavetable waveform.
* Areas to check:
  * phase step calculation for wavetable waveform.
  * phase step calculation for procedural waveform.
* Ideas to try:
  * Increase fractional precision of fixed point form: try 1.13.18.
  * Floating point implementation of generators & oscillators.
* Checked math for wave table generation; found minor rounding error (up to 1 sample out) - not really going to
  make a difference.
* Checked wavetable vs procedural phase step calculations:
  * Normalised against sample rate and maximum phase values.
  * Noticed that normalised procedural phase step < wavetable phase step - consistent with pitch difference.
  * Checked procedural phase step calculation - getting rounding error due to fixed point calculation order.
    * Corrected order, and now pitches match.
 
Overall Structure - first iteration
-----------------------------------
Per voice:
	2 x oscillator: waveform shape, phase, amplitude & relative pitch
	4 x envelope generator: ADSR params, one per oscillator, one for noise, one for filter
	1 x filter: cutoff, resonance
	
Global:
	2 x LFO: waveform shape, amplitude, pitch, phase, where output connects (OSC A/B: pitch, amplitude)
	1 x Noise: type of noise

Processing: per time slice (128 samples @ 44Khz, i.e. ~3ms)
	* Global processing:
		* Generate slice's worth of noise samples
		* Calculate current value of each LFO based on mid-point of time slice 
		  (low freq: assume constant for timeslice, i.e. max freq is 333hz)
	* Per voice processing:
		* voice has a base pitch (note) and phase (since triggered)
		* voice has two oscillators that need mixing with noise
		* For each oscillator:
			* Calculate setup:
				* frequency: base_pitch + osc_relative_pitch + optional_lfo 
				* level: osc_amplitude * optional_lfo
				* phase: base_phase + osc_phase
			* Generate sample data:
				* From setup parameters applied to waveform.
				* Amplitude shaped by envelope generator updated in step.
				* First oscillator writes to buffer directly.
				* Second oscillator is mixed to buffer.
		* Mix noise samples to buffer, amplitude shaped by envelope generator updated in step. 
		* Iterate over buffer, applying filter shaped by envelope generator updated in step.	
	* Mix each voice buffer into output buffer.
	
Common concepts:
 * Sample buffer
 * Envelope generator
 * Waveform
 * Oscillator
 
Common operations:
 * Generate samples from waveform into buffer
 * Mix buffer into other buffer

What needs to be done to achieve this:
* Add more waveform shapes (square, triangle, pulse)
* Ensure waveform generation minimises aliasing
* Implement voice relative oscillators
* Implement LFOs
* Implement envelope generators
* Implement noise generation
* Implement filtering
* Implement generic buffer mixing
* Data-driven mechanism for configuring (i.e. patches)
* UI for displaying & editing patch settings
* Extend MIDI handling to read note on/off for voice triggering

Useful info:
* Filters:
  * http://www.musicdsp.org/showArchiveComment.php?ArchiveID=185
  * http://www.musicdsp.org/showArchiveComment.php?ArchiveID=29
* Noise generation:
  * http://www.musicdsp.org/showArchiveComment.php?ArchiveID=216
  * http://www.musicdsp.org/showArchiveComment.php?ArchiveID=217

Bandlimited Waves
-----------------
Tried out bandlimited saw wave by summing sine waves (see http://www.musicdsp.org/files/waveforms.txt and
http://hackmeopen.com/2010/11/bandlimited-wavetable-synthesis) - seemed to reduce aliasing a bit, but not
a lot because of wide range of playback frequencies.

Aliasing reduced further by creating wavetable of higher pitched note; however this made lower pitches
"duller".

Profiling Notes
---------------
Tried out gperftools cpu profiling, in debug and release - with monophonic/polyphonic, wavetable/procedural.

Generally it appeared that the most significant of my own code was waveform output, be it wavetable or
procedural. 

Hard to compare the two different approaches as testing was somewhat ad-hoc; need to make clearly controlled
test cases (voice count & setup) and probably best capturing own metrics (time measurements, e.g. spent in
waveform generation, spend waiting for ALSA, etc.). Also use ProfilerStart() and ProfilerStop() to limit code
profiled (i.e. main loop only).

Biggest time overall in all profiling tests was in ioctl (between 30% and 45%). Semaphore wait and post
were also quite high; each around similar time to that spent in waveform output.

Tightening up profiling to just around the main loop shifted the goalposts; when doing polyphonic wavetable
output, nearly half CPU time is spent in the output mixing, then ~20% in ioctl, then ~9% in the non-mixed
output. With strictly defined testing config (voice setup, test duration) this would be a basis for
measuring impact of optimisation efforts.

Envelope Generator Notes
------------------------
Basic operation:
* Note is triggered: starts an envelope generator instance
  * Start time set to zero; output level set to zero; state set to ATTACK
* Time stepped with note playing
  * Envelope time advanced and checked against next state conditions; advance state if time outside bounds
  * Calculate normalised output level for envelope
  * Scale master volume by level and set on oscillator
* ADSR style envelopes
  * States have:
    * Starting level: 0-1
    * Ending level:   0-1
    * Duration:		  0 to infinity
    
Rendering Notes
---------------
Model of operation:
* Rendering runs on its own thread
* Other code (e.g. synth engine) posts rendering events to rendering thread
* Rendering thread will pick those up and execute them
  * Potentially up to a limit (time based, count based)
  * Then executes a buffer swap and goes to sleep until more events ready

Rendering events:
* Output waveform chunk
* Parameter change

Response to events:
* Waveform chunk:
  * Render waveform chunk into a buffer image until image full
  * Blit image to screen prior to screen flip
* Parameter change:
  * Map parameter to a display element
  * Trigger display element re-render with updated parameter value

Performance experiences:
* 32-bit pixel format is slow: get hits on both the blit (vgWritePixels) and waveform rendering. ~4fps in debug.
* 16-bit pixel format is better: twice as fast (~8fps in debug), about 25% of render time spent on the blit.
  E.g. exec 3075ms, idle 9373ms, frame rate 62.5fps when just doing blit.
  E.g. exec 12854ms, idle 27ms, frame rate 9fps when just rendering silence.
  E.g. exec 14878ms, idle 25ms, frame rate 7.98fps when rendering full waveform (debug + 8 voices).
* Rendering as sample-to-sample line segments was massively faster than bitmap form, when minimising OpenVG API calls
  through buffering:
  E.g. exec 3960ms, idle 13112ms, frame rate 62.5fps when rendering full waveform (debug + 8 voices).
  
Possible performance improvements for bitmaps (not likely to beat sample-to-sample line segments): 
* Inline the column filling.
* Reimplement render in assembly.
* Switch to row-major fill (i.e. linear memory access) and adjust size & rendering rotation accordingly.
* Average sample pairs and render half as much.

Observations:
* Audio/video sync not really there; amount of waveform rendered each visual frame varies continuously.
* Oscilloscope "window size" & AV sync (or lack) shows visual aliasing in how waveform appears depending on waveform frequency.
* Oscilloscope tuning achieved by controlling # samples rendered based on wavelength of tuned frequency.
  * Fixed sample rate and maximum renderable area mean some tuned wavelengths truncate the display.
  * Perfect tuning not yet achieved to inaccuracies in the system, e.g.
    * Rounding of wavelength in calculation
    * Inaccuracies in rendering and interpolation of wavetable data
    * Inaccuracies/bias in graphics refresh rate (in GPU, in LCD panel)

* Gfx improvements
  * Look at adjusting scale alongside wavelength to avoid truncated waveform display.
    * But note that using OpenVG to scale will also affect stroke as well as coordinates.
  * Refactor into a more general-purpose UI framework for better reusability.

 
