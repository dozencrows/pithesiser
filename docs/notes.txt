Raspberry Pi Synth Project
==========================

Underway
========

To do next
==========
* Resolve filtering & going to silence (eliminate bias & clicks)
* Improve filter math precision while maintaining performance
* Improve waveform quality
** Multi-waveform wavetables with bandlimited generation
** More variety of waveforms
** Pre-filtered procedural waveforms
** Cubic interpolation with table lookup
** Normalise range of phase accumulator across all waveform types
* Filters
  * Improve precision particularly at low values for Q and frequency (doesn't appear to be controller precision)

Blog ideas
==========

Ideas to try
============
Feature support:
 * Rendering:
   * Of settings
 * Better naming for oscillators & generators
 * Paired oscillators: modulation, mixing
 * Noise channel
 * Configuration of synth signal path & components via config file
 * Parameterise channel count/stride on sample buffer operations; proper stereo operations
 * Filtering:
  * Add overall gain control to LPF and HPF
  * Add more filter types (BPF, 1-POLE LPF & HPF, notch)
  * Drive filter param from envelope
  * Add per-voice filters
  * Render filter frequency response in UI
  * Add multiple filter stages
 * Pitch bend control
 * Modulation wheel control (linked with LFO)
 * LFO: vibrato, tremelo, with envelope
  * delay/two stage attack on envelopes
  * Multiple envelopes -> need smarter control management 
  * two independent LFOs; target separate outputs & separate parameters, but triggered in step.
 
Performance & quality:
 * Optimise WAV output
 ** Ensure good buffering
 ** Add own buffering, with lower priority thread writing out smaller chunks
 * Generalise gfx event system as it is being used to also save audio to disk 
 * Fixed point power function
 * Eliminate heap allocations during synth loop: (gfx event buffers)

Testing:
  * Whole synth tests: some form of sequencer?
  * Benchmarking
  * Clip testing

Performance:
 * Threading setup
   * Move "synth engine" to its own thread, allow main thread to be idle?
   * Set non-main thread priorities accordingly.

Advanced:
 * Phase Distortion control (simple linear)
 * Sample playback

Pitch Difference Issue (resolved)
----------------------
* Procedural generated waveform is pitching lower than wavetable waveform.
* Areas to check:
  * phase step calculation for wavetable waveform.
  * phase step calculation for procedural waveform.
* Ideas to try:
  * Increase fractional precision of fixed point form: try 1.13.18.
  * Floating point implementation of generators & oscillators.
* Checked math for wave table generation; found minor rounding error (up to 1 sample out) - not really going to
  make a difference.
* Checked wavetable vs procedural phase step calculations:
  * Normalised against sample rate and maximum phase values.
  * Noticed that normalised procedural phase step < wavetable phase step - consistent with pitch difference.
  * Checked procedural phase step calculation - getting rounding error due to fixed point calculation order.
    * Corrected order, and now pitches match.
 
Overall Structure - first iteration
-----------------------------------
Per voice:
	2 x oscillator: waveform shape, phase, amplitude & relative pitch
	4 x envelope generator: ADSR params, one per oscillator, one for noise, one for filter
	1 x filter: cutoff, resonance
	
Global:
	2 x LFO: waveform shape, amplitude, pitch, phase, where output connects (OSC A/B: pitch, amplitude)
	1 x Noise: type of noise

Processing: per time slice (128 samples @ 44Khz, i.e. ~3ms)
	* Global processing:
		* Generate slice's worth of noise samples
		* Calculate current value of each LFO based on mid-point of time slice 
		  (low freq: assume constant for timeslice, i.e. max freq is 333hz)
	* Per voice processing:
		* voice has a base pitch (note) and phase (since triggered)
		* voice has two oscillators that need mixing with noise
		* For each oscillator:
			* Calculate setup:
				* frequency: base_pitch + osc_relative_pitch + optional_lfo 
				* level: osc_amplitude * optional_lfo
				* phase: base_phase + osc_phase
			* Generate sample data:
				* From setup parameters applied to waveform.
				* Amplitude shaped by envelope generator updated in step.
				* First oscillator writes to buffer directly.
				* Second oscillator is mixed to buffer.
		* Mix noise samples to buffer, amplitude shaped by envelope generator updated in step. 
		* Iterate over buffer, applying filter shaped by envelope generator updated in step.	
	* Mix each voice buffer into output buffer.
	
Common concepts:
 * Sample buffer
 * Envelope generator
 * Waveform
 * Oscillator
 
Common operations:
 * Generate samples from waveform into buffer
 * Mix buffer into other buffer

What needs to be done to achieve this:
* Add more waveform shapes (square, triangle, pulse)
* Ensure waveform generation minimises aliasing
* Implement voice relative oscillators
* Implement noise generation
* Implement generic buffer mixing
* UI for displaying & editing patch settings
* STARTED: Implement filtering
* DONE: Data-driven mechanism for configuring (i.e. patches)
* DONE: Extend MIDI handling to read note on/off for voice triggering
* DONE: Implement LFOs
* DONE: Implement envelope generators

Useful info:
* Filters:
  * http://www.musicdsp.org/showArchiveComment.php?ArchiveID=185
  * http://www.musicdsp.org/showArchiveComment.php?ArchiveID=29
  * http://zynaddsubfx.git.sourceforge.net/git/gitweb.cgi?p=zynaddsubfx/zynaddsubfx;a=tree;f=src/DSP;h=2e55bb52e4b767825394789fcdd55a87f705c925;hb=HEAD
  * https://ccrma.stanford.edu/~jos/filters/Elementary_Filter_Sections.html
  * http://www.musicdsp.org/files/Audio-EQ-Cookbook.txt
* Noise generation:
  * http://www.musicdsp.org/showArchiveComment.php?ArchiveID=216
  * http://www.musicdsp.org/showArchiveComment.php?ArchiveID=217

Audo Quality Notes
==================
Bandlimited Waves
-----------------
Tried out bandlimited saw wave by summing sine waves (see http://www.musicdsp.org/files/waveforms.txt and
http://hackmeopen.com/2010/11/bandlimited-wavetable-synthesis) - seemed to reduce aliasing a bit, but not
a lot because of wide range of playback frequencies.

Aliasing reduced further by creating wavetable of higher pitched note; however this made lower pitches
"duller".

Audio Quality Issues
--------------------
* Clicks/steps when raising or lowering master volume.
This appears to be lack of resolution with basic MIDI controller allowing only 127 levels; certainly at higher levels,
the step between each is audible.
RESOLUTION: use 14-bit MIDI controller with greater precision & range - e.g. 4096 levels.

* Strange harmonics on sine wave with fast attack or release
Slowing down attack/release reduces this effect.
Might be caused by the fact that envelope processing precision is per buffer, not per sample; fast attack/release means
more dramatic changes in volume. Suggest trying interpolation of level across buffer.
RESOLUTION: interpolate changes in oscillator level across sample buffer eliminated this.

* Clicks when playing new notes, or when notes end
Suspect the automatic volume ducking is behind this; immediately lowering/raising volume level based on number of voices.
Current implementation scales the post-mix signal between two voices by 0.75 as each voice is processed, so scaling is
automatically proportional to number of voices
  * Switch to float and use interpolated normalisation?
  * Pre-calculate scale factor based on number of voices each buffer, and apply that interpolated between last & new value?
  * Pre-calculate scale factor for a voice at start based on number of voices & keep that for each voice?
  
Taking out the auto-ducking stopped the clicks (but increased likelihood of clipping).
Tried this method: mixed = A + B - (AB / SAMPLE_MAX) (see http://www.vttoth.com/CMS/index.php/technical-notes/68)
  * Close, but introduced strange harmonics on sine waves, and still clipped nastily at the bottom.
Other approaches would require structural changes:
  * Switch to 32 bit integer or float sample buffers, and convert prior to output. This would allow summing beyond 16 bit output limit
    which could be scaled & clipped in one pass.
  * Keep separate buffer for each oscillator, and mix with scale as a step after initial sample generation.
RESOLUTION: Factor modifier into oscillator level in main loop based on number of voices playing, utilise amplitude interpolation from
    		issue #2 to resolve it.

* Clicks/glitches when changing filter parameters
RESOLUTION: Tried to address this by filtering buffer twice when parameters change, with old and new values, then interpolate the resulting buffer
from old to new. This eliminated clicks & glitches.

* "Steps" in filter depth at low values of frequency
Printing out & analysing values in a spreadsheet shows this to be a precision issue; primarily the precision of the coefficients
used in filter_apply (and implicitly the precision used to calculate those coefficients). The challenge will be to increase this
precision suitably while avoiding overflows and keeping performance good.
- have been able to switch history & output in filter state to non-fixed point, which may give some headroom for this...

* Variable bias appearing in filtering at low frequencies
This looks like the "output" part of filter state gets "poisoned" somehow, which then persists in further use of the filter. If the filter
state is reset, this goes away (e.g. by changing filter type or explicitly clearing on silence).
Suspect that this bias comes from the last few non-zero samples just before the output goes to silence when all notes end - because of
the latency introduced by the filter, output silence happens a little later than voice silence. Temporarily addressed this by
clearing the filter state when all voices silent - eliminates the bias, but can cause clicks (worse with notes close to being completely
cut by the filter).
The logic of the filtering algorithm may still require explicitly resetting on silence, with interpolation to avoid clicks. 

* Strange harmonics on various waves, particularly at higher pitches
Likely general digital aliasing.
  * Try higher internal sample rate (e.g. 88kHz) and downsample on output (average)
  * Use more band-limited waveforms.
  * Use multi-sampled wave tables.
  * Use precalculated filtering.

Performance Notes
=================
Profiling Notes
---------------
Tried out gperftools cpu profiling, in debug and release - with monophonic/polyphonic, wavetable/procedural.

Generally it appeared that the most significant of my own code was waveform output, be it wavetable or
procedural. 

Hard to compare the two different approaches as testing was somewhat ad-hoc; need to make clearly controlled
test cases (voice count & setup) and probably best capturing own metrics (time measurements, e.g. spent in
waveform generation, spend waiting for ALSA, etc.). Also use ProfilerStart() and ProfilerStop() to limit code
profiled (i.e. main loop only).

Biggest time overall in all profiling tests was in ioctl (between 30% and 45%). Semaphore wait and post
were also quite high; each around similar time to that spent in waveform output.

Tightening up profiling to just around the main loop shifted the goalposts; when doing polyphonic wavetable
output, nearly half CPU time is spent in the output mixing, then ~20% in ioctl, then ~9% in the non-mixed
output. With strictly defined testing config (voice setup, test duration) this would be a basis for
measuring impact of optimisation efforts.

Added a basic framework for "offline" code timing tests, and tried out an initial "float v fixed" filter
implementation test. Interestingly this showed that the floating point version was nearly 3x faster than
fixed point!
* Study of fixed point code generated showed that the routine was a great deal bigger than the float version;
  suspect a lot of the 64 bit integer manipulation is behind this. Reducing precision of fixed pt may improve this.

Created test implementations of procedural sine and wavetable sine using floating point:
* Procedural around 15% to 30% faster with float
* Wavetable started off slower with float; but switching to fixed point phase accumulator, got around a 15% speedup
  * Looked like float to int conversion was the slowest part
* Originally had the float implementations as static functions in same module; when moving these to a separate
  module for reuse, that slowed down the float tests:
  * Procedural float now around 38% to 56% slower.
  * Wavetable float output only was ~1.5% slower, wavetable float mix still faster ~19% 
* As a test, put procedural float sine back in same module - both as static and non-static
  * Now about 3%-5% faster than fixed, but around 40% quicker than when in own module.
  * Looking at the generated code, the function-in-same-module configuration has inlined the code inside the loop,
    eliminating the call & cache overhead.
  * So this might suggest fixed point is a lot faster... however, rigging the test to make the fixed point code
    inline didn't show any significant performance change compared to calling it.
  * Maybe float version has better caching behaviour when inlined?

Note that this is across a large number of repeats (100k) in order to significant time values, so although this
should average out spikes, it might be influenced by other factors such as calling costs & caching.

Using a smaller number of repeats (128) shows very short times of sub 1ms to 2ms for the various. Generally fixed
pt <= float for these.
  
Created test implementation converting float to int16_t for output:
* Using simple multiply-&-cast, this was somewhat slow - around 0.03ms to 0.1ms for a single 128 sample stereo buffer.
  Not the end of the world, but one of the slowest operations (similar to unoptimised fixed point filter).
  
Made a simple performance model spreadsheet which suggested:
* Not much difference between float for wavetable model
* Procedural float oscillator is about 50% slower than fixed version
* No signal path took more than 0.2ms in the model; suggests there is plenty of headroom for more work
  (such as more envelopes, filters and oscillators, proper stereo).
  * Assume 50% error in timings suggests a max of 0.3ms - allowing for other operations (such as rendering) then could
    possibly push the synth to about 3x to 5x of current load (i.e. up to about 1.5ms of the 3ms needed for each buffer).
* Increasing filter count to one-per-voice plus global shows float implementation ahead
* Increasing oscillator count to two-per-voice puts fixed point ahead for procedural, but a little behind on wavetable.  

Suggested strategy:
* Try optimising fixed point filter to see if it can be got closer to float performance; if so, stay fixed point.
* If not, try optimising float->int conversion to reduce that.
* Also consider optimising float procedural oscillators.

Results:
* By reducing fixed point filter precision to 14 bits from 18, was able to make that implementation ~13% faster than
  the float form; strongly suggesting I stick with fixed point when scaling up functionality, as on average each
  functional stage is faster in fixed point. Where it isn't, the difference is small enough to be dwarfed by the need
  to convert float to integer for output.
* By changing the filter application calculation to only use fixed point for the calculation and not in storing
  the history or output values for later lookup, further performance was gained - now roughly ~45% faster than the
  float form. However may have to give some of this back in order to increase filter precision.    

Thread Timing
-------------
When introduced an "init" sysex message send for dumping controller snapshots during post-midi initialisation 
but before the main loop, early messages returned from this were dropped.

Attempted to yield main thread around sysex send - this didn't work in various places, but changed the timing a bit.
Attempted to synchronise midi thread initialisation with semaphore - this didn't work.
Added a usleep delay of 200ms after each per-channel sysex send - this worked.

Design Notes
============
Envelope Generator Notes
------------------------
Basic operation:
* Note is triggered: starts an envelope generator instance
  * Start time set to zero; output level set to zero; state set to ATTACK
* Time stepped with note playing
  * Envelope time advanced and checked against next state conditions; advance state if time outside bounds
  * Calculate normalised output level for envelope
  * Scale master volume by level and set on oscillator
* ADSR style envelopes
  * States have:
    * Starting level: 0-1
    * Ending level:   0-1
    * Duration:		  0 to infinity
    
Rendering Notes
---------------
Model of operation:
* Rendering runs on its own thread
* Other code (e.g. synth engine) posts rendering events to rendering thread
* Rendering thread will pick those up and execute them
  * Potentially up to a limit (time based, count based)
  * Then executes a buffer swap and goes to sleep until more events ready

Rendering events:
* Output waveform chunk
* Parameter change

Response to events:
* Waveform chunk:
  * Render waveform chunk into a buffer image until image full
  * Blit image to screen prior to screen flip
* Parameter change:
  * Map parameter to a display element
  * Trigger display element re-render with updated parameter value

Performance experiences:
* 32-bit pixel format is slow: get hits on both the blit (vgWritePixels) and waveform rendering. ~4fps in debug.
* 16-bit pixel format is better: twice as fast (~8fps in debug), about 25% of render time spent on the blit.
  E.g. exec 3075ms, idle 9373ms, frame rate 62.5fps when just doing blit.
  E.g. exec 12854ms, idle 27ms, frame rate 9fps when just rendering silence.
  E.g. exec 14878ms, idle 25ms, frame rate 7.98fps when rendering full waveform (debug + 8 voices).
* Rendering as sample-to-sample line segments was massively faster than bitmap form, when minimising OpenVG API calls
  through buffering:
  E.g. exec 3960ms, idle 13112ms, frame rate 62.5fps when rendering full waveform (debug + 8 voices).
  
Possible performance improvements for bitmaps (not likely to beat sample-to-sample line segments): 
* Inline the column filling.
* Reimplement render in assembly.
* Switch to row-major fill (i.e. linear memory access) and adjust size & rendering rotation accordingly.
* Average sample pairs and render half as much.

Observations:
* Audio/video sync not really there; amount of waveform rendered each visual frame varies continuously.
* Oscilloscope "window size" & AV sync (or lack) shows visual aliasing in how waveform appears depending on waveform frequency.
* Oscilloscope tuning achieved by controlling # samples rendered based on wavelength of tuned frequency.
  * Fixed sample rate and maximum renderable area mean some tuned wavelengths truncate the display.
  * Perfect tuning not yet achieved to inaccuracies in the system, e.g.
    * Rounding of wavelength in calculation
    * Inaccuracies in rendering and interpolation of wavetable data
    * Inaccuracies/bias in graphics refresh rate (in GPU, in LCD panel)

* Gfx improvements
  * Look at adjusting scale alongside wavelength to avoid truncated waveform display.
    * But note that using OpenVG to scale will also affect stroke as well as coordinates.
  * Refactor into a more general-purpose UI framework for better reusability.

LFO Notes
---------
LFO frequency is typically 20 Hz or less. For simplicity & optimisation, can assume that LFO value is constant across
each synthesis packet.
* Can reuse existing oscillator code with special "LFO" waveforms, and a "midpoint" output calculation for a synthesis packet.
* LFO application to volume is linear
* LFO application to pitch is exponential (2 raised to LFO amplitude)
* Useful to half both full waveforms (for pitch mostly) and positive only (biased) waveforms (for volume).
* LFO is reset when no notes are playing (or reset on first note after silence) but stays uniform for all notes when playing.

