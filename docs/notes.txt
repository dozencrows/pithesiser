Raspberry Pi Synth Project
==========================

Ideas to try
============
Feature support:
 * Better naming for oscillators & generators
 * Normalise range of phase accumulator across all waveform types
 * Multi-wave wavetable support 
 * Cubic interpolation with table lookup
 * Implementation of "whole synth" (see overall structure notes below)
 * Paired oscillators: modulation, mixing
 * Modulation tools:
   * Envelope generator
   * Amplitude modulation
   * Frequency modulation
   * LFOs
 * Noise channel
 * Filtering

Performance:
 * More profiling of waveform generation loops
 * Examine asm code generated for waveform gen loops; look for calls vs inlined code
 * Float based implementation (wavetable & procedural)
   * Rationales:
     * See if it produces output closer to sine for the approximate sine procedural
     * See if it produces less aliased output for saw wave (wavetable or procedural)

Advanced:
 * Phase Distortion control (simple linear)

Pitch Difference Issue
----------------------
* Procedural generated waveform is pitching lower than wavetable waveform.
* Areas to check:
  * phase step calculation for wavetable waveform.
  * phase step calculation for procedural waveform.
* Ideas to try:
  * Increase fractional precision of fixed point form: try 1.13.18.
  * Floating point implementation of generators & oscillators.
* Checked math for wave table generation; found minor rounding error (up to 1 sample out) - not really going to
  make a difference.
* Checked wavetable vs procedural phase step calculations:
  * Normalised against sample rate and maximum phase values.
  * Noticed that normalised procedural phase step < wavetable phase step - consistent with pitch difference.
  * Checked procedural phase step calculation - getting rounding error due to fixed point calculation order.
    * Corrected order, and now pitches match.
 
Overall Structure - first iteration
-----------------------------------
Per voice:
	2 x oscillator: waveform shape, phase, amplitude & relative pitch
	4 x envelope generator: ADSR params, one per oscillator, one for noise, one for filter
	1 x filter: cutoff, resonance
	
Global:
	2 x LFO: waveform shape, amplitude, pitch, phase, where output connects (OSC A/B: pitch, amplitude)
	1 x Noise: type of noise

Processing: per time slice (128 samples @ 44Khz, i.e. ~3ms)
	* Global processing:
		* Generate slice's worth of noise samples
		* Calculate current value of each LFO based on mid-point of time slice 
		  (low freq: assume constant for timeslice, i.e. max freq is 333hz)
	* Per voice processing:
		* voice has a base pitch (note) and phase (since triggered)
		* voice has two oscillators that need mixing with noise
		* For each oscillator:
			* Calculate setup:
				* frequency: base_pitch + osc_relative_pitch + optional_lfo 
				* level: osc_amplitude * optional_lfo
				* phase: base_phase + osc_phase
			* Generate sample data:
				* From setup parameters applied to waveform.
				* Amplitude shaped by envelope generator updated in step.
				* First oscillator writes to buffer directly.
				* Second oscillator is mixed to buffer.
		* Mix noise samples to buffer, amplitude shaped by envelope generator updated in step. 
		* Iterate over buffer, applying filter shaped by envelope generator updated in step.	
	* Mix each voice buffer into output buffer.
	
Common concepts:
 * Sample buffer
 * Envelope generator
 * Waveform
 * Oscillator
 
Common operations:
 * Generate samples from waveform into buffer
 * Mix buffer into other buffer

What needs to be done to achieve this:
* Add more waveform shapes (square, triangle, pulse)
* Ensure waveform generation minimises aliasing
* Implement voice relative oscillators
* Implement LFOs
* Implement envelope generators
* Implement noise generation
* Implement filtering
* Implement generic buffer mixing
* Data-driven mechanism for configuring (i.e. patches)
* UI for displaying & editing patch settings
* Extend MIDI handling to read note on/off for voice triggering

Useful info:
* Filters:
  * http://www.musicdsp.org/showArchiveComment.php?ArchiveID=185
  * http://www.musicdsp.org/showArchiveComment.php?ArchiveID=29
* Noise generation:
  * http://www.musicdsp.org/showArchiveComment.php?ArchiveID=216
  * http://www.musicdsp.org/showArchiveComment.php?ArchiveID=217

Bandlimited Waves
-----------------
Tried out bandlimited saw wave by summing sine waves (see http://www.musicdsp.org/files/waveforms.txt and
http://hackmeopen.com/2010/11/bandlimited-wavetable-synthesis) - seemed to reduce aliasing a bit, but not
a lot because of wide range of playback frequencies.

Aliasing reduced further by creating wavetable of higher pitched note; however this made lower pitches
"duller".

Profiling Notes
---------------
Tried out gperftools cpu profiling, in debug and release - with monophonic/polyphonic, wavetable/procedural.

Generally it appeared that the most significant of my own code was waveform output, be it wavetable or
procedural. 

Hard to compare the two different approaches as testing was somewhat ad-hoc; need to make clearly controlled
test cases (voice count & setup) and probably best capturing own metrics (time measurements, e.g. spent in
waveform generation, spend waiting for ALSA, etc.). Also use ProfilerStart() and ProfilerStop() to limit code
profiled (i.e. main loop only).

Biggest time overall in all profiling tests was in ioctl (between 30% and 45%). Semaphore wait and post
were also quite high; each around similar time to that spent in waveform output.

Tightening up profiling to just around the main loop shifted the goalposts; when doing polyphonic wavetable
output, nearly half CPU time is spent in the output mixing, then ~20% in ioctl, then ~9% in the non-mixed
output. With strictly defined testing config (voice setup, test duration) this would be a basis for
measuring impact of optimisation efforts.
  