Raspberry Pi Synth Project
==========================

Underway
========
* Re-enabling envelopes in new synth model:
  * normalised scale for editing, rescaling for output
    * filter freq
    * q
  * adjustable start and end values
  * Envelope release stage & voice lifetime

To do next
==========
* Modulation matrix to map LFO outputs onto filter parameters
* Data-driven settings:
  * migrate remaining synth controls to settings - midi control, rendering in UI, UI setup
  * loading & saving of settings (replace MIDI controller output saving)
  * default values for settings, when they're set and how the results are driven to the synth model at startup
* Modularise synth model init and processing
* Secondary LFO support
* Enrich filter controls
  * Optional voice pitch tracking
  * LFO control
* Improve filtering quality
  * Resolve filtering & going to silence (eliminate bias & clicks)
  * Investigate use of interpolation
  * Add another couple of bits precision to fixed point math in filters
* Improve waveform quality
  * Multi-waveform wavetables with bandlimited generation
  * More variety of waveforms
  * Pre-filtered procedural waveforms
  * Cubic interpolation with table lookup
  * Normalise range of phase accumulator across all waveform types
  
Done
====
 * Master volume & waveform as settings - midi control, rendering
 * Persisting MIDI controller outputs
 * Migrate standalone synth globals into central "synth model" structure
 * Indexed MIDI controls
   * Indexed envelope editing for 3 envelopes (volume, filter freq, filter q)
 * Relative continuous controller support
 * Relative continuous controllers for all envelope controls
 * Basic modulation matrix mechanism
 * LFO as mod matrix source, mapping to note pitch and/or volume as sink
 * Launchpad S MIDI control over modulation matrix 
 * Persist mod matrix setup and initialise controller to match on startup
 * Standardise logging on log4c with internal macros
 * Refactor voices & envelopes to decouple via callbacks in synth model
 * Add voice killing and active voice counting to synth model via voice callbacks
 * Sort out LFO reset - on first voice start from silence)
 * Modulation matrix to map envelope outputs onto different parameters

Blog ideas
==========

Ideas to try
============
Feature support:
 * Better naming for oscillators & generators
 * Paired oscillators: (cross) modulation, mixing, detuning
 * Noise channel
 * Configuration of synth signal path & components via config file
 * Parameterise channel count/stride on sample buffer operations; proper stereo operations
 * Filtering:
  * Add overall gain control to LPF and HPF
  * Add more filter types (BPF, 1-POLE LPF & HPF, notch)
  * Drive filter param from envelope
  * Add per-voice filters
  * Render filter frequency response in UI
  * Add multiple filter stages
 * Pitch bend control
 * Modulation wheel control (linked with LFO)
 * LFO: vibrato, tremelo, with envelope
  * delay/two stage attack on envelopes
  * Multiple envelopes -> need smarter control management 
  * two independent LFOs; target separate outputs & separate parameters, but triggered in step.
 * Pulse waveform
   * variable pulse width
   * LFO driving pulse width
 * Ring modulator
 * PiGlow
   * Alternative behaviours
 
Performance & quality:
 * Optimise WAV output
 ** Ensure good buffering
 ** Add own buffering, with lower priority thread writing out smaller chunks
 * Generalise gfx event system as it is being used to also save audio to disk 
 * Fixed point power function
 * Eliminate heap allocations during synth loop: (gfx event buffers)

Testing:
  * Whole synth tests: some form of sequencer?
  * Benchmarking
  * Clip testing

Performance:
 * Threading setup
   * Move "synth engine" to its own thread, allow main thread to be idle?
   * Set non-main thread priorities accordingly.

Advanced:
 * Phase Distortion control (simple linear)
 * Sample playback

Pitch Difference Issue (resolved)
----------------------
* Procedural generated waveform is pitching lower than wavetable waveform.
* Areas to check:
  * phase step calculation for wavetable waveform.
  * phase step calculation for procedural waveform.
* Ideas to try:
  * Increase fractional precision of fixed point form: try 1.13.18.
  * Floating point implementation of generators & oscillators.
* Checked math for wave table generation; found minor rounding error (up to 1 sample out) - not really going to
  make a difference.
* Checked wavetable vs procedural phase step calculations:
  * Normalised against sample rate and maximum phase values.
  * Noticed that normalised procedural phase step < wavetable phase step - consistent with pitch difference.
  * Checked procedural phase step calculation - getting rounding error due to fixed point calculation order.
    * Corrected order, and now pitches match.
 
Overall Structure - first iteration
-----------------------------------
Per voice:
	2 x oscillator: waveform shape, phase, amplitude & relative pitch
	4 x envelope generator: ADSR params, one per oscillator, one for noise, one for filter
	1 x filter: cutoff, resonance
	
Global:
	2 x LFO: waveform shape, amplitude, pitch, phase, where output connects (OSC A/B: pitch, amplitude)
	1 x Noise: type of noise

Processing: per time slice (128 samples @ 44Khz, i.e. ~3ms)
	* Global processing:
		* Generate slice's worth of noise samples
		* Calculate current value of each LFO based on mid-point of time slice 
		  (low freq: assume constant for timeslice, i.e. max freq is 333hz)
	* Per voice processing:
		* voice has a base pitch (note) and phase (since triggered)
		* voice has two oscillators that need mixing with noise
		* For each oscillator:
			* Calculate setup:
				* frequency: base_pitch + osc_relative_pitch + optional_lfo 
				* level: osc_amplitude * optional_lfo
				* phase: base_phase + osc_phase
			* Generate sample data:
				* From setup parameters applied to waveform.
				* Amplitude shaped by envelope generator updated in step.
				* First oscillator writes to buffer directly.
				* Second oscillator is mixed to buffer.
		* Mix noise samples to buffer, amplitude shaped by envelope generator updated in step. 
		* Iterate over buffer, applying filter shaped by envelope generator updated in step.	
	* Mix each voice buffer into output buffer.
	
Common concepts:
 * Sample buffer
 * Envelope generator
 * Waveform
 * Oscillator
 
Common operations:
 * Generate samples from waveform into buffer
 * Mix buffer into other buffer

What needs to be done to achieve this:
* Add more waveform shapes (square, triangle, pulse)
* Ensure waveform generation minimises aliasing
* Implement voice relative oscillators
* Implement noise generation
* Implement generic buffer mixing
* UI for displaying & editing patch settings
* STARTED: Implement filtering
* DONE: Data-driven mechanism for configuring (i.e. patches)
* DONE: Extend MIDI handling to read note on/off for voice triggering
* DONE: Implement LFOs
* DONE: Implement envelope generators

Useful info:
* Filters:
  * http://www.musicdsp.org/showArchiveComment.php?ArchiveID=185
  * http://www.musicdsp.org/showArchiveComment.php?ArchiveID=29
  * http://zynaddsubfx.git.sourceforge.net/git/gitweb.cgi?p=zynaddsubfx/zynaddsubfx;a=tree;f=src/DSP;h=2e55bb52e4b767825394789fcdd55a87f705c925;hb=HEAD
  * https://ccrma.stanford.edu/~jos/filters/Elementary_Filter_Sections.html
  * http://www.musicdsp.org/files/Audio-EQ-Cookbook.txt
* Noise generation:
  * http://www.musicdsp.org/showArchiveComment.php?ArchiveID=216
  * http://www.musicdsp.org/showArchiveComment.php?ArchiveID=217

Audo Quality Notes
==================
Bandlimited Waves
-----------------
Tried out bandlimited saw wave by summing sine waves (see http://www.musicdsp.org/files/waveforms.txt and
http://hackmeopen.com/2010/11/bandlimited-wavetable-synthesis) - seemed to reduce aliasing a bit, but not
a lot because of wide range of playback frequencies.

Aliasing reduced further by creating wavetable of higher pitched note; however this made lower pitches
"duller".

Audio Quality Issues
--------------------
* Clicks/steps when raising or lowering master volume.
This appears to be lack of resolution with basic MIDI controller allowing only 127 levels; certainly at higher levels,
the step between each is audible.
RESOLUTION: use 14-bit MIDI controller with greater precision & range - e.g. 4096 levels.

* Strange harmonics on sine wave with fast attack or release
Slowing down attack/release reduces this effect.
Might be caused by the fact that envelope processing precision is per buffer, not per sample; fast attack/release means
more dramatic changes in volume. Suggest trying interpolation of level across buffer.
RESOLUTION: interpolate changes in oscillator level across sample buffer eliminated this.

* Clicks when playing new notes, or when notes end
Suspect the automatic volume ducking is behind this; immediately lowering/raising volume level based on number of voices.
Current implementation scales the post-mix signal between two voices by 0.75 as each voice is processed, so scaling is
automatically proportional to number of voices
  * Switch to float and use interpolated normalisation?
  * Pre-calculate scale factor based on number of voices each buffer, and apply that interpolated between last & new value?
  * Pre-calculate scale factor for a voice at start based on number of voices & keep that for each voice?
  
Taking out the auto-ducking stopped the clicks (but increased likelihood of clipping).
Tried this method: mixed = A + B - (AB / SAMPLE_MAX) (see http://www.vttoth.com/CMS/index.php/technical-notes/68)
  * Close, but introduced strange harmonics on sine waves, and still clipped nastily at the bottom.
Other approaches would require structural changes:
  * Switch to 32 bit integer or float sample buffers, and convert prior to output. This would allow summing beyond 16 bit output limit
    which could be scaled & clipped in one pass.
  * Keep separate buffer for each oscillator, and mix with scale as a step after initial sample generation.
RESOLUTION: Factor modifier into oscillator level in main loop based on number of voices playing, utilise amplitude interpolation from
    		issue #2 to resolve it.

* Clicks/glitches when changing filter parameters
RESOLUTION: Tried to address this by filtering buffer twice when parameters change, with old and new values, then interpolate the resulting buffer
from old to new. This eliminated clicks & glitches.

* "Steps" in filter depth at low values of frequency
Printing out & analysing values in a spreadsheet shows this to be a precision issue; primarily the precision of the coefficients
used in filter_apply (and implicitly the precision used to calculate those coefficients). The challenge will be to increase this
precision suitably while avoiding overflows and keeping performance good.
- have been able to switch history & output in filter state to non-fixed point, which may give some headroom for this...
- have been able to implement 32-bit sized filtering code in assembler that's quicker than optimised C, giving some CPU headroom for increasing precision
- study of values in LPF & HPF filtering suggests the following approach:
  - coefficients can be stored as 32 bit values up to fixed precision of 29 before causing an overflow
  - coefficient calculations in filter_update need to use fixed_wide_t (64 bits) throughout to avoid overflow
  - ARM smull and smlal instructions can be used for multiply/multiply-accumulate operations - require additional register for high 32 bits
  - conversion from 64 bit back to 16 bit samples can be done using shift on low 32 bits or'd with rotated & masked high 32 bits after adding "0.5" to round.
Carried out test implementation of actual filter loops using 64 bit operations as above, using r14 (lr) temporarily for high word. This worked, but was
slower - non interpolated test was about 38% slower, interpolated was about 13% slower. This was across 2560 iterations, where there seemed to be a consistent
5ms longer time for the higher precision versions. In absolute terms of a single iteration, the additional cost is only then about 2 microseconds.  

* Variable bias appearing in filtering at low frequencies
This looks like the "output" part of filter state gets "poisoned" somehow, which then persists in further use of the filter. If the filter
state is reset, this goes away (e.g. by changing filter type or explicitly clearing on silence).
Suspect that this bias comes from the last few non-zero samples just before the output goes to silence when all notes end - because of
the latency introduced by the filter, output silence happens a little later than voice silence. Temporarily addressed this by
clearing the filter state when all voices silent - eliminates the bias, but can cause clicks (worse with notes close to being completely
cut by the filter).
The logic of the filtering algorithm may still require explicitly resetting on silence, with interpolation to avoid clicks. 

* Glitches & instability with an HPF with low cutoff frequency when changing Q or frequency 

* Strange harmonics on various waves, particularly at higher pitches
Likely general digital aliasing.
  * Try higher internal sample rate (e.g. 88kHz) and downsample on output (average)
  * Use more band-limited waveforms.
  * Use multi-sampled wave tables.
  * Use precalculated filtering.

Performance Notes
=================
Profiling Notes
---------------
Tried out gperftools cpu profiling, in debug and release - with monophonic/polyphonic, wavetable/procedural.

Generally it appeared that the most significant of my own code was waveform output, be it wavetable or
procedural. 

Hard to compare the two different approaches as testing was somewhat ad-hoc; need to make clearly controlled
test cases (voice count & setup) and probably best capturing own metrics (time measurements, e.g. spent in
waveform generation, spend waiting for ALSA, etc.). Also use ProfilerStart() and ProfilerStop() to limit code
profiled (i.e. main loop only).

Biggest time overall in all profiling tests was in ioctl (between 30% and 45%). Semaphore wait and post
were also quite high; each around similar time to that spent in waveform output.

Tightening up profiling to just around the main loop shifted the goalposts; when doing polyphonic wavetable
output, nearly half CPU time is spent in the output mixing, then ~20% in ioctl, then ~9% in the non-mixed
output. With strictly defined testing config (voice setup, test duration) this would be a basis for
measuring impact of optimisation efforts.

Added a basic framework for "offline" code timing tests, and tried out an initial "float v fixed" filter
implementation test. Interestingly this showed that the floating point version was nearly 3x faster than
fixed point!
* Study of fixed point code generated showed that the routine was a great deal bigger than the float version;
  suspect a lot of the 64 bit integer manipulation is behind this. Reducing precision of fixed pt may improve this.

Created test implementations of procedural sine and wavetable sine using floating point:
* Procedural around 15% to 30% faster with float
* Wavetable started off slower with float; but switching to fixed point phase accumulator, got around a 15% speedup
  * Looked like float to int conversion was the slowest part
* Originally had the float implementations as static functions in same module; when moving these to a separate
  module for reuse, that slowed down the float tests:
  * Procedural float now around 38% to 56% slower.
  * Wavetable float output only was ~1.5% slower, wavetable float mix still faster ~19% 
* As a test, put procedural float sine back in same module - both as static and non-static
  * Now about 3%-5% faster than fixed, but around 40% quicker than when in own module.
  * Looking at the generated code, the function-in-same-module configuration has inlined the code inside the loop,
    eliminating the call & cache overhead.
  * So this might suggest fixed point is a lot faster... however, rigging the test to make the fixed point code
    inline didn't show any significant performance change compared to calling it.
  * Maybe float version has better caching behaviour when inlined?

Note that this is across a large number of repeats (100k) in order to significant time values, so although this
should average out spikes, it might be influenced by other factors such as calling costs & caching.

Using a smaller number of repeats (128) shows very short times of sub 1ms to 2ms for the various. Generally fixed
pt <= float for these.
  
Created test implementation converting float to int16_t for output:
* Using simple multiply-&-cast, this was somewhat slow - around 0.03ms to 0.1ms for a single 128 sample stereo buffer.
  Not the end of the world, but one of the slowest operations (similar to unoptimised fixed point filter).
  
Made a simple performance model spreadsheet which suggested:
* Not much difference between float for wavetable model
* Procedural float oscillator is about 50% slower than fixed version
* No signal path took more than 0.2ms in the model; suggests there is plenty of headroom for more work
  (such as more envelopes, filters and oscillators, proper stereo).
  * Assume 50% error in timings suggests a max of 0.3ms - allowing for other operations (such as rendering) then could
    possibly push the synth to about 3x to 5x of current load (i.e. up to about 1.5ms of the 3ms needed for each buffer).
* Increasing filter count to one-per-voice plus global shows float implementation ahead
* Increasing oscillator count to two-per-voice puts fixed point ahead for procedural, but a little behind on wavetable.  

Suggested strategy:
* Try optimising fixed point filter to see if it can be got closer to float performance; if so, stay fixed point.
* If not, try optimising float->int conversion to reduce that.
* Also consider optimising float procedural oscillators.

Results:
* By reducing fixed point filter precision to 14 bits from 18, was able to make that implementation ~13% faster than
  the float form; strongly suggesting I stick with fixed point when scaling up functionality, as on average each
  functional stage is faster in fixed point. Where it isn't, the difference is small enough to be dwarfed by the need
  to convert float to integer for output.
* By changing the filter application calculation to only use fixed point for the calculation and not in storing
  the history or output values for later lookup, further performance was gained - now roughly ~45% faster than the
  float form. However may have to give some of this back in order to increase filter precision.    

Thread Timing
-------------
When introduced an "init" sysex message send for dumping controller snapshots during post-midi initialisation 
but before the main loop, early messages returned from this were dropped.

Attempted to yield main thread around sysex send - this didn't work in various places, but changed the timing a bit.
Attempted to synchronise midi thread initialisation with semaphore - this didn't work.
Added a usleep delay of 200ms after each per-channel sysex send - this worked.

Design Notes
============
Envelope Generator Notes
------------------------
Basic operation:
* Note is triggered: starts an envelope generator instance
  * Start time set to zero; output level set to zero; state set to ATTACK
* Time stepped with note playing
  * Envelope time advanced and checked against next state conditions; advance state if time outside bounds
  * Calculate normalised output level for envelope
  * Scale master volume by level and set on oscillator
* ADSR style envelopes
  * States have:
    * Starting level: 0-1
    * Ending level:   0-1
    * Duration:		  0 to infinity

Generalising envelopes:
* Normalised levels in definitions - affects:
  * Rendering: envelope peak value controls scale already.
  * Controllers: output min/max for all envelope level controls should just be set to normalised range. 
  * Model update: inside voice_update - apply scaling to normalised values here.
* Replace explicit rendering IDs with generalised names
* Replace explicit binding of envelopes to model parameters with modulation matrix
  * current binding is on voice_init call
    
Rendering Notes
---------------
Model of operation:
* Rendering runs on its own thread
* Other code (e.g. synth engine) posts rendering events to rendering thread
* Rendering thread will pick those up and execute them
  * Potentially up to a limit (time based, count based)
  * Then executes a buffer swap and goes to sleep until more events ready

Rendering events:
* Output waveform chunk
* Parameter change

Response to events:
* Waveform chunk:
  * Render waveform chunk into a buffer image until image full
  * Blit image to screen prior to screen flip
* Parameter change:
  * Map parameter to a display element
  * Trigger display element re-render with updated parameter value

Performance experiences:
* 32-bit pixel format is slow: get hits on both the blit (vgWritePixels) and waveform rendering. ~4fps in debug.
* 16-bit pixel format is better: twice as fast (~8fps in debug), about 25% of render time spent on the blit.
  E.g. exec 3075ms, idle 9373ms, frame rate 62.5fps when just doing blit.
  E.g. exec 12854ms, idle 27ms, frame rate 9fps when just rendering silence.
  E.g. exec 14878ms, idle 25ms, frame rate 7.98fps when rendering full waveform (debug + 8 voices).
* Rendering as sample-to-sample line segments was massively faster than bitmap form, when minimising OpenVG API calls
  through buffering:
  E.g. exec 3960ms, idle 13112ms, frame rate 62.5fps when rendering full waveform (debug + 8 voices).
  
Possible performance improvements for bitmaps (not likely to beat sample-to-sample line segments): 
* Inline the column filling.
* Reimplement render in assembly.
* Switch to row-major fill (i.e. linear memory access) and adjust size & rendering rotation accordingly.
* Average sample pairs and render half as much.

Observations:
* Audio/video sync not really there; amount of waveform rendered each visual frame varies continuously.
* Oscilloscope "window size" & AV sync (or lack) shows visual aliasing in how waveform appears depending on waveform frequency.
* Oscilloscope tuning achieved by controlling # samples rendered based on wavelength of tuned frequency.
  * Fixed sample rate and maximum renderable area mean some tuned wavelengths truncate the display.
  * Perfect tuning not yet achieved to inaccuracies in the system, e.g.
    * Rounding of wavelength in calculation
    * Inaccuracies in rendering and interpolation of wavetable data
    * Inaccuracies/bias in graphics refresh rate (in GPU, in LCD panel)

* Gfx improvements
  * Look at adjusting scale alongside wavelength to avoid truncated waveform display.
    * But note that using OpenVG to scale will also affect stroke as well as coordinates.
  * Refactor into a more general-purpose UI framework for better reusability.

LFO Notes
---------
LFO frequency is typically 20 Hz or less. For simplicity & optimisation, can assume that LFO value is constant across
each synthesis packet.
* Can reuse existing oscillator code with special "LFO" waveforms, and a "midpoint" output calculation for a synthesis packet.
* LFO application to volume is linear
* LFO application to pitch is exponential (2 raised to LFO amplitude)
* Useful to half both full waveforms (for pitch mostly) and positive only (biased) waveforms (for volume).
* LFO is reset when no notes are playing (or reset on first note after silence) but stays uniform for all notes when playing.

MIDI Notes
----------
* Relative controller modes:
  * rel 1: 3rd byte is 0x01 for increment, 0x7f for decrement (2's complement in 7 bits)
  * rel 2: 3rd byte is 0x41 for increment, 0x3f for decrement (offset from $40)
  * rel 3: 3rd byte is 0x01 for increment, 0x41 for decrement (bit 6 = sign, bits 0-5 = value)

Modulation Matrix Notes
-----------------------
First pass:
  * Sources: LFO, envelope generators x 3
  * Sinks: voice amplitude, voice pitch, filter frequency, filter Q, LFO amplitude, LFO frequency 
  * Source notes:
    * LFO - varies sink around base value
    * Envelope - offsets sink base value directly
    * Definition:
      * name
      * generation function (generates normalised value for source at given time)
  * Sink notes
    * Definition:
      * name
      * base level function (calculates base value for sink at given time)
      * model update function (sets value into appropriate part of synth model)
  * Operation
    * sources are iterated and generation functions called
    * sinks are iterated and base level functions are called
    * connected source-sink pairs are iterated:
      * source application function called for sink
      * resulting value passed to sink model update function
  * Data structures
    * source definitions
    * sink definitions
    * source-sink connections
  * Persistence
    * Save and load the set of source-sink connections
    * Use text files & libconfig syntax - can parse on synth startup
  * MIDI control via Launchpad S
    * Column = source, row = sink
      * Define source/column and sink/row mappings in config file
    * Initialisation
      * Parse config file to register source names with columns, sink names with rows
      * Send command to turn off all LEDs
      * Iterate source-sink connections, determining note number (via column & row) and
        send note on with velocity to set colour - e.g. 
        * 0x90 0xnn 0x0c for off
        * 0x90 0xnn 0x3c for bright green
        * 0x90 0xnn 0x0f for bright red
        * 0x90 0xnn 0x2f for bright orange
        * 0x90 0xnn 0x3e for bright yellow
    * When button pressed (note on event with velocity 127)
      * Convert note value to column & row: row = note / 16, column = note % 16
      * Determine source & sink names
      * If connection for the pair exists
        * Delete connection
        * Send note on to turn off LED (see above)
      * Else
        * Create connection
        * Send note on to turn on LED (see above) 
  * Challenges
    * Playing notes as sinks means that actual true number of sinks is variable - this needs to be handled
      in the general voice update/matrix update code.
    * Significant change to voice processing code
      * Currently envelope stepping, envelope application, lfo application are integral in voice_update - will
        need to split this and voice update apart.
      * Suggest piecewise approach - start with say just LFO as source, and note volume and pitch as sinks.
        Then move on to envelopes and filters.
    * Making matrix changes smooth as possible while notes are playing
      * E.g.:
        * reset & restart envelope when turning envelope on
        * resetting envelope effects when turning envelope off (hopefully base value calculation will do this)
  * Supporting envelopes applied to array (e.g. voices) and single sinks (e.g. lfo) ...
    * Indexed sources idea:
      * Replace direct source value extraction with a callback that takes an id argument.
      * Voice sink model update functions use the callback, passing voice number as id.
      * Callback for envelopes uses index to look up an instance:
        * Locates envelope instance and returns current value.
        * Use a "global" id value for an envelope that runs across all playing voices, e.g. for LFO.
      * Extend voice module to add callback mechanism for voice state changes (start, release).
        * Also provide interface to drive voice state changes.
      * Extend envelope module to add callback mechanism for stage changes.
      * Rework voice/envelope integration to decouple their existence and integrate behaviour via the callbacks.
        * Use synth model for ownership of voice and envelope instances, and binding and implementation of callbacks.
        * Voice start callback can start envelopes in synth model - model or mod matrix can also step envelopes as part of update.
        * Provide a case also for "global" envelopes that track oldest triggered voice, and reset when all voices stop.
        * Note - may not need envelope callbacks (originally for stopping voice when envelope completes, but now no guarantee of
          envelope and also potential for multiple envelopes affecting other factors)
    * Assignment idea:
      * Two types of mod matrix source - direct value source & definition sources.
      * Direct value - work as now.
      * Definition - change properties on the sink when matrix connection made or unmade
        * Breaks abstraction, tightens coupling - need to know what the definition is of, and what to set it on as part of the sink.
        * Could get abstraction by having the definition as a binary blob of data with size, and an associated string label for the type.
        * Sink maps label onto portions of its internal data:
          * Copies the binary blob provided when connection made.
          * Resets the definition when disconnection made.
        * Avoids abstracting currently coupled behaviour such as envelope release and completion, as this still happens in the voice.
        * Avoids indices and time values in mod matrix interface.
		* Need to add envelope instances & related code to each sink as well as voices (e.g. lfo) - could be a lot of extra stuff...      

Voice Lifetime
--------------
The main issue here is release time.
* "Note-off" triggers release
  * If no envelope assigned to amplitude, then note should stop dead.
  * If envelope assigned to amplitude, envelope should switch to release stage and play out - at end, note should stop.
  * Other amplitude modifiers should apply, but not affect the duration of release.
    * E.g. LFO should still change amplitude, but not cause note to stop early or continue past release end.

Current implementation uses note state (NOTE_ENDING) and oscillator level. This isn't enough information as in each update:
* Sink base update sets oscillator amplitude to max
* Sink model update applies modifiers to amplitude
* If note is NOTE_ENDING, voice update will only stop note when oscillator amplitude is zero
  * base update prevents this
  * LFO modulation doesn't quite get to zero consistently, so will prevent this also.

The tricky part is maintaining the flexibility of the modulation matrix model, where an initial base value is set and then modified
while correctly determining release.

I think what is needed is a more explicit control of voice lifetime control:
* Notes still have states: NOT PLAYING, PLAYING, ENDING
* "Note-off" changes state to ENDING
  * voice callback explicitly checks for envelope assignment to amplitude modulation
    * if found, switches all envelopes to release stage
      * notes will persist in ENDING state until amplitude goes to zero
    * else kills voice immediately

Development Notes
=================
Initially I found single stepping when remote debugging to be flakey - it would regularly lock up, and I would have to kill the
remote process and start again.

However, turning on "non stop" GDB which allows unsuspended threads to keep running seems to have solved this!
